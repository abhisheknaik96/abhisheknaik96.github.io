---
layout : post
title : Solving the Rubik's Cube Without Human Knowledge
date : 2018-06-27
time : +2021
permalink : posts/RubiksSolver
tags : summary
---

[[arXiv;1805.08966](https://arxiv.org/pdf/1805.08966v1.pdf)] 
---

Here I summarise this remarkable paper that caught my eye, owing to my interest in cubing. 

### TL;DR

- Uses Autodidactic Iteration (ADI), a novel reinforcement learning algorithm that is able to teach itself how to solve the Rubik's Cube with no human assistance, to augment asynchronous MCTS. 
- Solves 100% of randomly scrambled cubes while achieving a median solve length of 30 moves — less than or equal to solvers that employ human domain knowledge. 


### Motivation

- Starting from one of the $$4.3 \times 10^{19}$$ states to reach the single goal state, algorithms like A3C could never reach the goal state in order to get a reward/learning signal.
- Autodidactic Iteration (ADI) trains the value function through an iterative supervised learning process.

### Methodology

They use a 2-step method - first training a DNN to estimate the values of each state, and then using it to guide MCTS. Once the network is trained, the policy is used to reduce breadth and the value is used to reduce depth in the MCTS.     

1. ADI is an iterative supervised learning procedure which trains a deep neural network $$f_{\theta}(s)$$ with parameters $$\theta$$ which takes an input state $$s$$ and outputs a value and policy pair $$(v, p)$$.
    1. In each iteration of ADI, training samples for $$f_{\theta}$$ are generated by starting from the solved cube. This ensures that some training inputs will be close enough to have a positive reward when performing a shallow search. With each sample, the number of scrambles it took to generate it, $$D(x_i)$$, is stored.
    2. Targets $$Y_i = (y_{v_i} , y_{p_i})$$ are then created by performing a depth-1 breadth-first search (BFS) from each training sample.
    3. The current value network is used to estimate each child’s value. 
    4. The value target for each sample is the maximum value and reward of each of its children, and the policy target is the action which led to this maximal value.

2. The MCTS Solver
    1. Tree expansion - for each timestep $$t$$, an action is selected by choosing $$A_t = \arg\max_{a} U_{s_t}(a) + Q_{s_t}(a)$$ where $$U_{s_t}(a) = c P_{s_t}(a) {\sqrt{\sum_{a'} N_{s_t}(a')}} / (1 + N_{s_t}(a))$$, and $$Q_{s_t}(a) = {W_{s_t}(a)} - L_{s_t}(a)$$
    2. Leaf expansion - Once a leaf node is reached, the state is expanded by adding the children of $$s_\tau$$, with appropriate memory initializations.
    3. The value and policy are computed and backprop-ed till the root.
    4. If $$s_{\tau}$$ is the solved state, then the tree $$T$$ of the simulation is extracted and converted into an undirected graph with unit weights. A full breath-first search is then applied on $$T$$ to find the shortest predicted path from the starting state to solution. Alternatively, one could naively use the sequence path from $$0$$ to $$\tau$$, but this produces longer solutions.
        

Like AlphaGo Zero, the same network with different policy and value heads is trained.

### Results

1. DeepCube solves all 640 randomly scrambled cubes with an hour. An hour.
2. Achieves a median solve length of 30 moves!

Interesting further analysis :
1. reveals that it learns the widely-used $$aba^{-1}$$ pattern 
2. shows that it learns something akin to [the Lars Petrus strategy](http://lar5.com/cube/) of solving the $$2\times2\times2$$ sub-cube first before orienting the corners and edges and shifting them to the appropriate places  
    
## Notes

- God's number - 26 - the upper bound on the number of moves required to solve *any* cube orientation.
